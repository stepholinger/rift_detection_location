{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c19f1-3bb7-4790-9f09-32de3814222f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# notebook to run all the seismic analysis and processing in Olinger et al 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cced7bbf-dbec-4a70-a773-9aec1010f766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import time\n",
    "import h5py\n",
    "import obspy\n",
    "from obspy.clients.fdsn.mass_downloader import RectangularDomain, Restrictions, MassDownloader\n",
    "import types\n",
    "import pyasdf\n",
    "import pickle\n",
    "import rasterio\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "from detection.stalta_detector import stalta_detector\n",
    "from detection.master_event_correlation import correlate_master, threshold_detections, plot_catalog\n",
    "from detection.template_match import make_templates, template_match, detection_timeseries\n",
    "from clustering.clustering import get_input_waveforms, load_waveform, cluster_events, plot_clusters\n",
    "from location.compute_backazimuths import compute_backazimuths, get_detection_times\n",
    "from figures.figures import (plot_backazimuths_on_imagery, transform_imagery, get_station_coordinates, \n",
    "                             get_station_grid_locations, plot_daily_events_and_gps, plot_weekly_events_and_gps)\n",
    "from stacking.stacking import get_rotated_waveforms, get_stacks\n",
    "from stacking.tilt_stacking import get_rotated_tilt_waveforms, get_tilt_stack\n",
    "from gps.gps import gps_ice_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb008c8-a097-4d4f-b4ef-05c189ffb128",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Download data and remove instrumental response\n",
    "\n",
    "'''\n",
    "\n",
    "# specify path to save data- directories will be created here for MSEED and XML\n",
    "# default is the provided \"data\" folder, but change the path below to put data elsewhere on your filesystem\n",
    "#data_path = \"/media/Data/Data/PIG/MSEED/\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "# pull data from IRIS servers\n",
    "domain = RectangularDomain(minlatitude=-90, maxlatitude=90, minlongitude=-180, maxlongitude=180.0)\n",
    "\n",
    "restrictions = Restrictions(\n",
    "    starttime=obspy.UTCDateTime(2012, 1, 1),\n",
    "    endtime=obspy.UTCDateTime(2014, 1, 1),\n",
    "    chunklength_in_sec=86400,\n",
    "    network=\"XC\", station=\"PIG2\", location=\"\", channel=\"HH*\",\n",
    "    location_priorities = [\"01\",],\n",
    "    channel_priorities = [\"HHZ\",\"HHN\",\"HHE\"],\n",
    "    reject_channels_with_gaps=False,\n",
    "    minimum_length=0.0)\n",
    "\n",
    "mdl = MassDownloader(providers=[\"IRIS\"])\n",
    "\n",
    "mdl.download(\n",
    "    domain=domain,\n",
    "    restrictions=restrictions, \n",
    "    mseed_storage=data_path + \"MSEED/raw/\", \n",
    "    stationxml_storage=(data_path + \"XML/\"))\n",
    "\n",
    "#set frequency band for response removal\n",
    "pre_filt = [0.0005,0.001,45,50]\n",
    "\n",
    "#make a list of all files in the raw folder\n",
    "raw_files = glob.glob(data_path + \"MSEED/raw/*\", recursive=True)\n",
    "raw_files.sort()\n",
    "\n",
    "#loop through all raw files\n",
    "for f in raw_files:\n",
    "\n",
    "    #start timer\n",
    "    t = time.time()\n",
    "\n",
    "    #read in one data file\n",
    "    st = obspy.read(f)\n",
    "\n",
    "    #grab a couple useful variables from file metadata\n",
    "    station = st[0].stats.station\n",
    "    channel = st[0].stats.channel\n",
    "    start_date = str(st[0].stats.starttime).split(\"T\")[0]\n",
    "\n",
    "    #specify output filename format\n",
    "    out_path = data_path + \"MSEED/no_IR/\" + station + \"/\" + channel + \"/\" \n",
    "    out_file = out_path + start_date + \".\" + station + \".\" + channel + \".no_IR.MSEED\"\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    \n",
    "    #preprocess file\n",
    "    st.detrend(\"demean\")\n",
    "    st.detrend(\"linear\")\n",
    "    st.taper(max_percentage=0.00025, max_length=20.)\n",
    "\n",
    "    #read correct stationXML file\n",
    "    XML_path = glob.glob(data_path + \"XML/*\" + station + \"*.xml\")[0]\n",
    "    inv = obspy.read_inventory(XML_path)\n",
    "\n",
    "    #remove instrument response\n",
    "    st.remove_response(inventory=inv,pre_filt=pre_filt,output=\"VEL\")\n",
    "\n",
    "    #write new file\n",
    "    st.write(out_file,format='MSEED')\n",
    "\n",
    "    #end timer\n",
    "    run_time = time.time() - t\n",
    "\n",
    "    #give some output to check progress\n",
    "    print(\"Response removed from \" + f + \" in \" + str(run_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687464b7-dc54-4381-a1d3-19f4827c8997",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Run 2-band sta-lta detector to find template events\n",
    "\n",
    "'''\n",
    "\n",
    "# initialize detection parameter object and set parameters for stalta\n",
    "d = types.SimpleNamespace()\n",
    "d.low_freq = [0.01,1]\n",
    "d.high_freq = [1,10]\n",
    "d.tolerance = 120\n",
    "d.low_thresh_on = 8\n",
    "d.low_thresh_off = 2\n",
    "d.high_thresh_on = 20\n",
    "d.high_thresh_off = 2\n",
    "d.sta_len = 10\n",
    "d.lta_len = 300\n",
    "d.num_stations = 3\n",
    "\n",
    "# specify window to pull template around detection in seconds\n",
    "d.buffer = [2*60,3*60]\n",
    "\n",
    "# specify paths to data and metadata\n",
    "d.data_path = \"data/MSEED/no_IR/\"\n",
    "d.xml_path = \"data/XML/\"\n",
    "\n",
    "# select number of processors\n",
    "d.n_procs = 10\n",
    "\n",
    "# run the detector and save ASDF dataset with waveforms and metadata\n",
    "stalta_detector(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628712b-7bf2-4120-94db-3159e696d5b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Cross correlate master event with the rest of the detections to identify best templates\n",
    "\n",
    "'''\n",
    "\n",
    "# set normalized cross correlation threshold for making templates\n",
    "threshold = 0.9\n",
    "\n",
    "# load catalog in read-only mode\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/stalta_catalog.h5\",mode='r')\n",
    "\n",
    "# get waveforms for a single station \n",
    "waveforms = ds.waveforms.XC_PIG2.stream\n",
    "\n",
    "# filter waveforms\n",
    "freq = [0.05,1]\n",
    "waveforms.taper(max_percentage=0.1, max_length=30.)\n",
    "waveforms.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "\n",
    "# set master event for correlation after plotting to see if it looks dispersive and has high SNR\n",
    "for station in ds.ifilter(ds.q.starttime == \"2013-07-10T00:33:54.290001Z\",ds.q.station==\"PIG2\"):\n",
    "    master_event = station.stream\n",
    "    master_event.taper(max_percentage=0.1, max_length=30.)\n",
    "    master_event.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "    master_event.plot()\n",
    "    \n",
    "# correlate master with all other waveforms\n",
    "correlation_coefficients, shifts = correlate_master(master_event,waveforms,master_event[0].stats.npts,\"stalta\")\n",
    "\n",
    "# apply threshold to choose best templates and make plots\n",
    "threshold_detections(waveforms,correlation_coefficients,shifts,threshold)\n",
    "\n",
    "# close ASDF dataset\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95ef1f-e612-4890-b04d-c701460f449d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Make ASDF dataset of templates for use in multiband template matching procedure\n",
    "\n",
    "''' \n",
    "\n",
    "# set frequency limits for low and high frequency templates\n",
    "low_freq = [0.05,1]\n",
    "high_freq = [1,10]\n",
    "\n",
    "# load catalog in read-only mode\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/stalta_catalog.h5\",mode='r')\n",
    "\n",
    "# load list of template times\n",
    "template_file = h5py.File('outputs/detections/template_times.h5', 'r')\n",
    "template_times = list(template_file['timestamps'])\n",
    "template_file.close()\n",
    "\n",
    "# make all the templates\n",
    "make_templates(ds,template_times,high_freq,low_freq,xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b7efa-7657-41fd-9bed-e5676b553211",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot each template to verify quality\n",
    "\n",
    "''' \n",
    "\n",
    "# open ASDF dataset \n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/templates.h5\",mode='r')\n",
    "\n",
    "# make plot of both frequency bands \n",
    "for event in ds.events:\n",
    "    for station in ds.ifilter(ds.q.event == event):\n",
    "        station.low.plot()\n",
    "        station.high.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3cd51c-349b-4fd4-8ef3-7dae8fe1fed2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Delete a few templates that are unsuitable for use in template matching\n",
    "\n",
    "''' \n",
    "\n",
    "# delete a specific station's record of a particular event\n",
    "for station in ds.ifilter(ds.q.starttime == \"2013-03-23T05:46:55.24\",ds.q.station == \"PIG4\"):\n",
    "    del station.low\n",
    "    del station.high\n",
    "    \n",
    "# close the dataset\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deab018-2aac-4f70-9ee4-71da024dc163",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Run 2-band template matching detection procedure\n",
    "\n",
    "'''\n",
    "\n",
    "# initialize detection parameter object and set parameters for template matching\n",
    "d = types.SimpleNamespace()\n",
    "d.low_freq = [0.05,1]\n",
    "d.high_freq = [1,10]\n",
    "d.tolerance = 120\n",
    "d.low_thresh_on = 0.3\n",
    "d.low_thresh_off = 0.1\n",
    "d.high_thresh_on = 0.2\n",
    "d.high_thresh_off = 0.1\n",
    "d.num_stations = 3\n",
    "\n",
    "# specify window to pull template around detection in seconds\n",
    "d.buffer = [2*60,3*60]\n",
    "\n",
    "# specify paths to data and metadata\n",
    "d.data_path = \"data/MSEED/no_IR/\"\n",
    "d.xml_path = \"data/XML/\"\n",
    "                            \n",
    "# select number of processors\n",
    "d.n_procs = 18\n",
    "\n",
    "# run the parallel template matching code\n",
    "template_match(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80f66e-14f8-47a7-981e-aace13a4b0b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plots the results of template matching\n",
    "\n",
    "'''\n",
    "\n",
    "# load catalog\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='a')\n",
    "\n",
    "# make basic histogram of detection times\n",
    "detection_timeseries(ds,\"outputs/detections/template_matching_timeseries.png\")\n",
    "\n",
    "# get waveforms for a single station \n",
    "ds.single_item_read_limit_in_mb=6000\n",
    "waveforms = ds.waveforms.XC_PIG2.stream\n",
    "\n",
    "# filter waveforms\n",
    "freq = [0.05,1]\n",
    "waveforms.taper(max_percentage=0.1, max_length=30.)\n",
    "waveforms.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "\n",
    "# set master event for correlation after plotting to see if it looks dispersive and has high SNR\n",
    "for station in ds.ifilter(ds.q.starttime == \"2013-07-10T00:33:55.240001Z\" ,ds.q.station==\"PIG2\"):\n",
    "    master_event = station.stream\n",
    "    master_event.taper(max_percentage=0.1, max_length=30.)\n",
    "    master_event.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "    master_event.plot()\n",
    "    \n",
    "# correlate master with all other waveforms\n",
    "correlation_coefficients, shifts = correlate_master(master_event,waveforms,master_event[0].stats.npts,\"detection/template_matching\")\n",
    "\n",
    "# make plot of detected event waveforms\n",
    "plot_catalog(waveforms,correlation_coefficients,shifts)\n",
    "\n",
    "# close ASDF dataset\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fa78f-e8ff-45f2-95ea-2cb8286969fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get waveforms for k-shape clustering\n",
    "\n",
    "'''\n",
    "\n",
    "# load catalog in read-only mode\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "\n",
    "# initialize  parameter object and set parameters for clustering\n",
    "c = types.SimpleNamespace()\n",
    "c.station = \"PIG2\"\n",
    "c.component_order = [\"Z\",\"N\",\"E\"]\n",
    "c.freq = [0.05,5]\n",
    "c.num_clusters = 2\n",
    "c.trace_length = 500\n",
    "#c.data_path = \"data/MSEED/no_IR/\"\n",
    "c.data_path = \"/media/Data/Data/PIG/MSEED/noIR/\"\n",
    "\n",
    "# get matrix of 3D waveforms from a single station\n",
    "waveforms = get_input_waveforms(ds,c)\n",
    "\n",
    "# save waveforms\n",
    "waveform_file = h5py.File(\"outputs/clustering/input_waveforms.h5\",'w')\n",
    "waveform_file.create_dataset(\"waveforms\",data=waveforms)\n",
    "waveform_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f170d-9200-449e-bb86-00032ab55517",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Cluster the catalog with k-shape\n",
    "\n",
    "'''\n",
    "\n",
    "# load catalog in read-only mode\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "\n",
    "# initialize  parameter object and set parameters for clustering\n",
    "c = types.SimpleNamespace()\n",
    "c.station = \"PIG2\"\n",
    "c.component_order = [\"Z\",\"N\",\"E\"]\n",
    "c.freq = [0.05,5]\n",
    "c.num_clusters = 2\n",
    "c.trace_length = 500\n",
    "#c.data_path = \"data/MSEED/no_IR/\"\n",
    "c.data_path = \"/media/Data/Data/PIG/MSEED/noIR/\"\n",
    "\n",
    "# load waveforms\n",
    "waveform_file = h5py.File(\"outputs/clustering/input_waveforms_high_frequency.h5\",'r')\n",
    "waveforms = np.array(waveform_file['waveforms'])\n",
    "waveform_file.close()\n",
    "\n",
    "# run clustering\n",
    "cluster_events(c,waveforms)\n",
    "\n",
    "# close dataset\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bf53e-2fed-44d6-930a-4369b853f274",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Cross-correlate clustered waveforms with the centroid of their cluster and plot\n",
    "\n",
    "'''\n",
    "\n",
    "# load waveforms\n",
    "waveform_file = h5py.File(\"outputs/clustering/input_waveforms.h5\",'r')\n",
    "waveforms = np.array(waveform_file['waveforms'])\n",
    "waveform_file.close()\n",
    "\n",
    "# load clustering results\n",
    "cluster_file = h5py.File(\"outputs/clustering/\" + str(c.num_clusters) + \"_cluster_results.h5\",\"r\")\n",
    "predictions = np.array(list(cluster_file[\"cluster_index\"]))\n",
    "centroids = list(cluster_file[\"centroids\"])\n",
    "cluster_file.close()\n",
    "\n",
    "# shift centroids to center (kshape sometimes outputs centroids that are shifted to the sides input window which can cause further alignment issues later). The shift values for the correction done here are obtained simply by plotting the centroids. \n",
    "centroids[0] = np.concatenate((centroids[0].ravel()[550:],np.zeros(550)))\n",
    "centroids[1] = np.concatenate((np.zeros(400),centroids[1].ravel()[:-400]))\n",
    "\n",
    "# load catalog\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "\n",
    "for cluster in range(len(centroids)):\n",
    "    \n",
    "    # get obspy stream with centroid\n",
    "    master_event = obspy.Stream(obspy.Trace(centroids[cluster]))\n",
    "\n",
    "    # put 3-component waves used for clustering in obspy streams\n",
    "    waveform_stream = obspy.Stream([])\n",
    "    for waveform in waveforms[predictions == cluster]:\n",
    "        waveform_stream += obspy.Trace(waveform)\n",
    "    \n",
    "    # correlate waves in each cluster with their centroid\n",
    "    correlation_coefficients, shifts = correlate_master(master_event,waveform_stream,math.floor(master_event[0].stats.npts/3),\"clustering/cluster_\"+str(cluster))\n",
    "\n",
    "    # read correlation results\n",
    "    correlation_file = h5py.File(\"outputs/clustering/cluster_\" + str(cluster) + \"_correlations.h5\",'r')\n",
    "    correlation_coefficients = np.array(correlation_file['correlation_coefficients'])\n",
    "    shifts = np.array(correlation_file['shifts'])\n",
    "    correlation_file.close()\n",
    "    \n",
    "    # make plots of waveforms from each cluster\n",
    "    plot_clusters(c,cluster,centroids[cluster],waveforms[predictions == cluster],correlation_coefficients,shifts)\n",
    "    \n",
    "    # make basic histogram of detection times for this cluster\n",
    "    detection_timeseries(ds,\"outputs/clustering/cluster_\" + str(cluster) + \"_timeseries.png\",cluster,predictions)\n",
    "    \n",
    "# close ASDF dataset\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94176ab-692b-498e-9bff-0b0143f5b05c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Compute backazimuths for all events in the catalog\n",
    "\n",
    "'''\n",
    "\n",
    "# initialize location parameter object and set parameters for backazimuth computation\n",
    "l = types.SimpleNamespace()\n",
    "l.win_len = 50\n",
    "l.slide = 5\n",
    "l.trace_len = 500\n",
    "l.num_steps = int((l.trace_len-l.win_len)/l.slide)+1\n",
    "l.stations = [\"PIG1\",\"PIG2\",\"PIG3\",\"PIG4\",\"PIG5\"]\n",
    "l.network = \"XC\"\n",
    "\n",
    "# load catalog\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "l.detection_times = get_detection_times(ds)\n",
    "\n",
    "# set the coordinate system in which we will do all grid-based calculations\n",
    "l.crs = \"EPSG:3245\"\n",
    "\n",
    "# set signal-to-noise ratio for throwing out stations in backazimuth computation\n",
    "l.snr_threshold = 0\n",
    "\n",
    "# set sta/lta ratio for throwing out individual windows in backazimuth computation\n",
    "l.stalta_threshold = 1\n",
    "\n",
    "# specify method for correcting pca components \n",
    "l.pca_correction = \"distance\"\n",
    "l.centroid = \"fixed\"\n",
    "\n",
    "# specify parameters for cross correlation based determination of station of first arrival\n",
    "l.max_shift = 1000\n",
    "l.freq = [0.05,1]\n",
    "l.fs = 100\n",
    "\n",
    "# specify paths to data and me tadata\n",
    "#l.data_path = \"data/MSEED/no_IR/\"\n",
    "#l.xml_path = \"data/XML/\"\n",
    "l.data_path = \"/media/Data/Data/PIG/MSEED/noIR/\"\n",
    "l.xml_path = \"/media/Data/Data/PIG/XML/HH/\"\n",
    "l.filename = \"outputs/locations/event_backazimuths_\" + '_'.join(l.stations) + \"_\" + l.pca_correction + \"_pca_\" + l.centroid + \"_centroid_long_window\"\n",
    "l.n_procs = 10\n",
    "\n",
    "# run the backazimuth code\n",
    "b = compute_backazimuths(l)\n",
    "\n",
    "# save the output\n",
    "baz_file = open(l.filename + \".pickle\", \"wb\")\n",
    "pickle.dump(b, baz_file)\n",
    "baz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a76ac3-c163-4c30-b48a-a64ca97e06a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Make the backazimuth figure in Olinger et al, 2021\n",
    "\n",
    "'''\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open(\"outputs/locations/event_backazimuths_PIG1_PIG2_PIG3_PIG4_PIG5_distance_pca_fixed_centroid.pickle\", \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "baz_file.close()\n",
    "\n",
    "# get backazimuths of only the events in the dispersive cluster (cluster 0)\n",
    "backazimuths = b.backazimuths\n",
    "\n",
    "# load clustering results\n",
    "cluster_file = h5py.File(\"outputs/clustering/2_cluster_results.h5\",\"r\")\n",
    "predictions = np.array(list(cluster_file[\"cluster_index\"]))\n",
    "centroids = list(cluster_file[\"centroids\"])\n",
    "cluster_file.close()\n",
    "\n",
    "# get station locations and array centroids\n",
    "station_lon_lat_coords = get_station_coordinates(\"data/XML/\")\n",
    "station_grid_coords = get_station_grid_locations(station_lon_lat_coords,\"epsg:3245\")\n",
    "array_centroid = np.mean(station_grid_coords,axis=0)\n",
    "\n",
    "# open LANDSAT imagery file\n",
    "file = \"data/imagery/LC08_L1GT_001113_20131012_20170429_01_T2_B4.TIF\"\n",
    "transform_imagery(file,'epsg:3245')\n",
    "\n",
    "# set where to split the distribution and which color to use for each\n",
    "color_bounds = [[0,120],[180,280],[280,360]]\n",
    "colors = [\"#7570b3\",\"#d95f02\",\"#1b9e77\"]\n",
    "\n",
    "# make the figure\n",
    "plot_backazimuths_on_imagery(backazimuths[predictions == 0],array_centroid,station_grid_coords,color_bounds,colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9a722-24ba-4a1d-b9d2-af17b91b33bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get rotated velocity seismograms for use in plotting and modeling\n",
    "\n",
    "'''\n",
    "\n",
    "# load catalog in read-only mode and extract times for each event in the dataset\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "detection_times = []\n",
    "for event in ds.events:\n",
    "    detection_times.append(event.origins[0].time.datetime)\n",
    "detection_times = np.array(detection_times)\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open('outputs/locations/event_backazimuths_PIG1_PIG2_PIG3_PIG4_PIG5_distance_pca_fixed_centroid.pickle', \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "backazimuths = b.backazimuths\n",
    "baz_file.close()\n",
    "\n",
    "# load clustering results\n",
    "cluster_file = h5py.File(\"outputs/clustering/2_cluster_results.h5\",\"r\")\n",
    "predictions = np.array(list(cluster_file[\"cluster_index\"]))\n",
    "cluster_file.close()\n",
    "\n",
    "# initialize  parameter object and set parameters for clustering\n",
    "r = types.SimpleNamespace()\n",
    "r.station = \"PIG2\"\n",
    "r.channel = \"HH*\"\n",
    "r.component_order = [\"Z\",\"R\",\"T\"]\n",
    "r.num_clusters = 2\n",
    "r.trace_length = 500\n",
    "r.backazimuths = backazimuths[predictions == 0]\n",
    "r.detection_times = detection_times[predictions == 0]\n",
    "#r.data_path = \"data/MSEED/no_IR/\"\n",
    "r.data_path = \"/media/Data/Data/PIG/MSEED/noIR/\"\n",
    "r.n_procs = 15\n",
    "\n",
    "# get matrix of 3D waveforms from a single station on first frequency band\n",
    "r.freq = [0.01,1]\n",
    "waveforms_stack = get_rotated_waveforms(r)\n",
    "\n",
    "# get matrix of 3D waveforms from a single station on second frequency band\n",
    "r.freq = [0.05,1]\n",
    "waveforms_corr = get_rotated_waveforms(r)\n",
    "\n",
    "# save waveforms\n",
    "waveform_file = h5py.File(\"outputs/stacking/rotated_waveforms_PIG2_0.01Hz.h5\",'w')\n",
    "waveform_file.create_dataset(\"waveforms_0.01-1Hz\",data=waveforms_stack)\n",
    "waveform_file.create_dataset(\"waveforms_0.05-1Hz\",data=waveforms_corr)\n",
    "waveform_file.close()\n",
    "\n",
    "# correct a few parameters for tilt waveform retreival\n",
    "r.data_path = \"/media/Data/Data/PIG/MSEED/raw/\"\n",
    "r.xml_path = \"/media/Data/Data/PIG/XML/HH/\"\n",
    "\n",
    "# get matrix of tilt waveforms from a single station on first frequency band\n",
    "r.freq = [0.01,1]\n",
    "waveforms_stack = get_rotated_tilt_waveforms(r)\n",
    "\n",
    "# get matrix of tilt waveforms from a single station on second frequency band\n",
    "r.freq = [0.05,1]\n",
    "waveforms_corr = get_rotated_tilt_waveforms(r)\n",
    "\n",
    "# save waveforms\n",
    "waveform_file = h5py.File(\"outputs/stacking/rotated_tilt_waveforms_PIG2_0.01Hz.h5\",'w')\n",
    "waveform_file.create_dataset(\"waveforms_0.01-1Hz\",data=waveforms_stack)\n",
    "waveform_file.create_dataset(\"waveforms_0.05-1Hz\",data=waveforms_corr)\n",
    "waveform_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190f0ede-1ac4-4bb6-b575-d20d395060a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get rotated tilt seismograms for use in modeling\n",
    "\n",
    "'''\n",
    "\n",
    "# load catalog in read-only mode and extract times for each event in the dataset\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "detection_times = []\n",
    "for event in ds.events:\n",
    "    detection_times.append(event.origins[0].time.datetime)\n",
    "detection_times = np.array(detection_times)\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open('outputs/locations/event_backazimuths_PIG1_PIG2_PIG3_PIG4_PIG5_distance_pca_fixed_centroid.pickle', \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "backazimuths = b.backazimuths\n",
    "baz_file.close()\n",
    "\n",
    "# load clustering results\n",
    "cluster_file = h5py.File(\"outputs/clustering/2_cluster_results.h5\",\"r\")\n",
    "predictions = np.array(list(cluster_file[\"cluster_index\"]))\n",
    "cluster_file.close()\n",
    "\n",
    "# initialize  parameter object and set parameters for clustering\n",
    "r = types.SimpleNamespace()\n",
    "r.station = \"PIG2\"\n",
    "r.channel = \"HH*\"\n",
    "r.component_order = [\"Z\",\"R\",\"T\"]\n",
    "r.num_clusters = 2\n",
    "r.trace_length = 500\n",
    "r.backazimuths = backazimuths[predictions == 0]\n",
    "r.detection_times = detection_times[predictions == 0]\n",
    "#r.data_path = \"data/MSEED/no_IR/\"\n",
    "r.data_path = \"/media/Data/Data/PIG/MSEED/raw/\"\n",
    "r.xml_path = \"/media/Data/Data/PIG/XML/HH/\"\n",
    "r.n_procs = 15\n",
    "\n",
    "# get matrix of 3D waveforms from a single station on first frequency band\n",
    "r.freq = [0.01,1]\n",
    "waveforms_stack = get_rotated_tilt_waveforms(r)\n",
    "\n",
    "# get matrix of 3D waveforms from a single station on second frequency band\n",
    "r.freq = [0.05,1]\n",
    "waveforms_corr = get_rotated_tilt_waveforms(r)\n",
    "\n",
    "# save waveforms\n",
    "waveform_file = h5py.File(\"outputs/stacking/rotated_tilt_waveforms_PIG2_0.01Hz.h5\",'w')\n",
    "waveform_file.create_dataset(\"waveforms_0.01-1Hz\",data=waveforms_stack)\n",
    "waveform_file.create_dataset(\"waveforms_0.05-1Hz\",data=waveforms_corr)\n",
    "waveform_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a51af8d-c222-4f89-a541-e6c3c417ff71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get daily stacks of vertical and radial event waveforms for plotting and modeling \n",
    "\n",
    "'''\n",
    "\n",
    "# set some parameters\n",
    "freq = [0.05,1]\n",
    "trace_length = 500\n",
    "cluster = 0\n",
    "\n",
    "# load waveforms\n",
    "waveform_file = h5py.File(\"outputs/stacking/rotated_waveforms_PIG2_0.01Hz.h5\",'r')\n",
    "waveforms_stack = np.array(waveform_file['waveforms_0.01-1Hz'])\n",
    "waveforms_corr = np.array(waveform_file['waveforms_0.05-1Hz'])\n",
    "waveform_file.close()\n",
    "\n",
    "# load waveforms\n",
    "waveform_file = h5py.File(\"outputs/stacking/rotated_tilt_waveforms_PIG2_0.01Hz.h5\",'r')\n",
    "tilt_waveforms_stack = np.array(waveform_file['waveforms_0.01-1Hz'])\n",
    "tilt_waveforms_corr = np.array(waveform_file['waveforms_0.05-1Hz'])\n",
    "waveform_file.close()\n",
    "\n",
    "# load clustering results\n",
    "cluster_file = h5py.File(\"outputs/clustering/2_cluster_results.h5\",\"r\")\n",
    "predictions = np.array(list(cluster_file[\"cluster_index\"]))\n",
    "centroids = list(cluster_file[\"centroids\"])\n",
    "cluster_file.close()\n",
    "\n",
    "# read correlation results\n",
    "correlation_file = h5py.File(\"outputs/clustering/cluster_0_correlations.h5\",'r')\n",
    "correlation_coefficients = np.array(correlation_file['correlation_coefficients'])\n",
    "shifts = np.array(correlation_file['shifts'])\n",
    "correlation_file.close()\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open('outputs/locations/event_backazimuths_PIG1_PIG2_PIG3_PIG4_PIG5_distance_pca_fixed_centroid.pickle', \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "backazimuths = b.backazimuths\n",
    "baz_file.close()\n",
    "\n",
    "# load catalog\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "detection_dates = []\n",
    "for event in ds.events:\n",
    "    detection_dates.append(event.origins[0].time.datetime.date())\n",
    "\n",
    "# subset everything by the desired cluster\n",
    "detection_dates = np.array(detection_dates)[predictions == cluster]\n",
    "#waveforms = waveforms[predictions == cluster]\n",
    "backazimuths = backazimuths[predictions == cluster]\n",
    "baz_bounds = [[180,280],[280,360],[0,120]]\n",
    "\n",
    "# make daily waveform stacks for each spatial cluster\n",
    "bool_indices = np.logical_and(backazimuths>=180,backazimuths<280)\n",
    "rift_tip_daily_stacks,rift_tip_stack = get_stacks(waveforms_stack[bool_indices],waveforms_corr[bool_indices],detection_dates[bool_indices],correlation_coefficients[bool_indices],cluster,shifts[bool_indices],freq,trace_length)\n",
    "bool_indices = np.logical_and(backazimuths>=0,backazimuths<120)\n",
    "margin_daily_stacks,margin_stack = get_stacks(waveforms_stack[bool_indices],waveforms_corr[bool_indices],detection_dates[bool_indices],correlation_coefficients[bool_indices],cluster,shifts[bool_indices],freq,trace_length)\n",
    "bool_indices = np.logical_and(backazimuths>=280,backazimuths<360) \n",
    "rift_margin_daily_stacks,rift_margin_stack = get_stacks(waveforms_stack[bool_indices],waveforms_corr[bool_indices],detection_dates[bool_indices],correlation_coefficients[bool_indices],cluster,shifts[bool_indices],freq,trace_length)\n",
    "\n",
    "# save stacks\n",
    "stack_file = h5py.File(\"outputs/stacking/stacks_PIG2_0.01Hz.h5\",'w')\n",
    "stack_file.create_dataset(\"rift_tip_daily_stacks\",data=rift_tip_daily_stacks)\n",
    "stack_file.create_dataset(\"margin_daily_stacks\",data=margin_daily_stacks)\n",
    "stack_file.create_dataset(\"rift_margin_daily_stacks\",data=rift_margin_daily_stacks)\n",
    "stack_file.create_dataset(\"rift_tip_stack\",data=rift_tip_stack)\n",
    "stack_file.create_dataset(\"margin_stack\",data=margin_stack)\n",
    "stack_file.create_dataset(\"rift_margin_stack\",data=rift_margin_stack)\n",
    "stack_file.close()\n",
    "\n",
    "# get tilt stacks for each spatial cluster\n",
    "bool_indices = np.logical_and(backazimuths>=180,backazimuths<280)\n",
    "rift_tip_tilt_stack = get_tilt_stack(tilt_waveforms_stack[bool_indices],tilt_waveforms_corr[bool_indices],correlation_coefficients[bool_indices],cluster,shifts[bool_indices],freq,trace_length)\n",
    "bool_indices = np.logical_and(backazimuths>=0,backazimuths<120)\n",
    "margin_tilt_stack = get_tilt_stack(tilt_waveforms_stack[bool_indices],tilt_waveforms_corr[bool_indices],correlation_coefficients[bool_indices],cluster,shifts[bool_indices],freq,trace_length)\n",
    "bool_indices = np.logical_and(backazimuths>=280,backazimuths<360) \n",
    "rift_margin_tilt_stack = get_tilt_stack(tilt_waveforms_stack[bool_indices],tilt_waveforms_corr[bool_indices],correlation_coefficients[bool_indices],cluster,shifts[bool_indices],freq,trace_length)\n",
    "\n",
    "# save stacks\n",
    "stack_file = h5py.File(\"outputs/stacking/tilt_stacks_PIG2_0.01Hz.h5\",'w')\n",
    "stack_file.create_dataset(\"rift_tip_stack\",data=rift_tip_tilt_stack)\n",
    "stack_file.create_dataset(\"margin_stack\",data=margin_tilt_stack)\n",
    "stack_file.create_dataset(\"rift_margin_stack\",data=rift_margin_tilt_stack)\n",
    "stack_file.close()\n",
    "\n",
    "# close ASDF dataset\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b32421-0483-467c-8c51-09fbe9aa4c0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Estimate noise level in non-event windows so we can show that variability in event counts is not simply a function of detectability\n",
    "\n",
    "'''\n",
    "\n",
    "# load catalog\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "\n",
    "# get vertical waveforms for a single station \n",
    "ds.single_item_read_limit_in_mb=6000\n",
    "waveforms = ds.waveforms.XC_PIG2.stream\n",
    "waveforms.filter(\"bandpass\",freqmin=0.05,freqmax=1)\n",
    "component_waveforms = waveforms.select(component=\"Z\")\n",
    "\n",
    "# get the first minute of event event window (contains noise) and calculate noise rms\n",
    "date_vector = np.array([])\n",
    "noise_rms_vector = np.array([])\n",
    "for w in component_waveforms:\n",
    "    noise_window = w.trim(w.stats.endtime-60,w.stats.endtime)\n",
    "    squared_trace = np.square(np.array(noise_window.data,dtype='float64'))\n",
    "    noise_rms = np.sqrt(np.sum(squared_trace)/len(squared_trace))\n",
    "    noise_rms_vector = np.append(noise_rms_vector,noise_rms)\n",
    "    date_vector = np.append(date_vector,noise_window.stats.starttime.date)\n",
    "noise_date_vect = np.unique(date_vector)\n",
    "    \n",
    "# get weekly iterator\n",
    "weeks = np.arange(noise_date_vect[0],noise_date_vect[-1],timedelta(days=7)).astype(datetime)\n",
    "    \n",
    "# get average noise rms for each day\n",
    "noise_vect = np.array([])\n",
    "for w in range(len(weeks)-1):\n",
    "    bool_ind = np.logical_and(date_vector>=weeks[w].date(),date_vector<weeks[w+1].date())\n",
    "    mean_rms = np.mean(noise_rms_vector[bool_ind])\n",
    "    noise_vect = np.append(noise_vect,mean_rms)\n",
    "noise_vect = np.append(noise_vect,np.mean(noise_rms_vector[date_vector>weeks[-1].date()]))\n",
    "    \n",
    "# save result\n",
    "noise_file = open(\"outputs/figures/noise.pickle\", \"wb\")\n",
    "pickle.dump([weeks,noise_vect],noise_file)\n",
    "noise_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55127dd3-61b4-43d3-a806-b6469c40055b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "\n",
    "Get ice speed from pre-processed GPS solutions \n",
    "\n",
    "'''\n",
    "\n",
    "# select GPS source\n",
    "station = \"sow3\"\n",
    "\n",
    "# set some parameters for linearly interpolating ice speed data to replace spike artifacts from numerical differentiation\n",
    "spike_height = 2e-5\n",
    "kde_width = 2.5\n",
    "\n",
    "# get the gps ice speed curve\n",
    "gps_speed,gps_time_vect = gps_ice_speed(station,spike_height,kde_width)\n",
    "\n",
    "# get rid of some artifacts at the front of the data\n",
    "gps_speed = gps_speed[2:]\n",
    "gps_time_vect = gps_time_vect[2:]\n",
    "\n",
    "# design a lowpass filter- the gps ice speed we calculated are average values for each day, so fs = 1/86400\n",
    "fs = 1/86400\n",
    "ny = fs/2\n",
    "cutoff = fs/6\n",
    "b, a = butter(4, cutoff/ny, 'low')\n",
    "\n",
    "# lowpass filter the gps ice speed\n",
    "filt_gps_speed = filtfilt(b, a, gps_speed)\n",
    "\n",
    "# convert from m/s to m/yr\n",
    "filt_gps_speed = filt_gps_speed * 86400 * 365\n",
    "\n",
    "# save result\n",
    "gps_file = open(\"outputs/figures/gps.pickle\", \"wb\")\n",
    "pickle.dump([filt_gps_speed,gps_time_vect],gps_file)\n",
    "gps_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19ccea-b8b9-4d17-9cd3-f2dae55245c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot timeseries of dispersive cluster events with waveforms and GPS ice velocity\n",
    "\n",
    "'''\n",
    "\n",
    "# set some parameters for the plot\n",
    "cluster = 0\n",
    "num_clusters = 2\n",
    "\n",
    "# load catalog\n",
    "ds = pyasdf.ASDFDataSet(\"outputs/detections/template_matching_catalog.h5\",mode='r')\n",
    "\n",
    "# extract times for each event in the dataset\n",
    "detection_times = []\n",
    "for event in ds.events:\n",
    "    detection_times.append(event.origins[0].time.datetime)\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open('outputs/locations/event_backazimuths_PIG1_PIG2_PIG3_PIG4_PIG5_distance_pca_fixed_centroid.pickle', \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "backazimuths = b.backazimuths\n",
    "baz_file.close()\n",
    "    \n",
    "# load clustering results\n",
    "cluster_file = h5py.File(\"outputs/clustering/\" + str(num_clusters) + \"_cluster_results.h5\",\"r\")\n",
    "predictions = np.array(list(cluster_file[\"cluster_index\"]))\n",
    "centroids = list(cluster_file[\"centroids\"])\n",
    "cluster_file.close()\n",
    "\n",
    "# read daily stacks of waveforms\n",
    "home_dir = str(pathlib.Path().absolute())\n",
    "stack_file = h5py.File(\"outputs/stacking/stacks.h5\",'r')\n",
    "rift_tip_daily_stacks = np.array(stack_file[\"rift_tip_daily_stacks\"])\n",
    "rift_margin_daily_stacks = np.array(stack_file[\"rift_margin_daily_stacks\"])\n",
    "ne_margin_daily_stacks = np.array(stack_file[\"ne_margin_daily_stacks\"])\n",
    "daily_stacks = [rift_tip_daily_stacks,rift_margin_daily_stacks,ne_margin_daily_stacks]\n",
    "rift_tip_stack = np.array(stack_file[\"rift_tip_stack\"])\n",
    "rift_margin_stack = np.array(stack_file[\"rift_margin_stack\"])\n",
    "ne_margin_stack = np.array(stack_file[\"ne_margin_stack\"])\n",
    "stacks = [rift_tip_stack,rift_margin_stack,ne_margin_stack]\n",
    "stack_file.close()\n",
    "\n",
    "# load noise\n",
    "noise_file = open(\"outputs/figures/noise.pickle\", \"rb\")\n",
    "noise_date_vect,noise_vect = pickle.load(noise_file)\n",
    "noise_file.close()\n",
    "\n",
    "# load gps\n",
    "gps_file = open(\"outputs/figures/gps.pickle\", \"rb\")\n",
    "filt_gps_speed,gps_time_vect = pickle.load(gps_file)\n",
    "noise_file.close()\n",
    "\n",
    "# set where to split the distribution and which color to use for each\n",
    "baz_bounds = [[180,280],[280,360],[0,120]]\n",
    "colors = [\"#d95f02\",\"#1b9e77\",\"#7570b3\"]\n",
    "\n",
    "# make the plot\n",
    "plot_weekly_events_and_gps(filt_gps_speed,gps_time_vect,noise_vect,noise_date_vect,np.array(detection_times)[predictions==cluster],daily_stacks,stacks,colors,baz_bounds,backazimuths[predictions==cluster])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
